\documentclass[../proyecto.tex]{memoir}
\begin{document}
\thispagestyle{empty}

\begin{center}
  {\large\bfseries \ProjectTitle}\\
\end{center}
  \begin{center}
  	
  \AuthorName\\
  \vspace{0.7cm}
  \noindent{\textbf{Palabras clave}: Teorema No Free Lunch, Deep Learning, Backpropagation, Gradiente Descendente, redes neuronales, redes neuronales convolucionales, algoritmos evolutivos, Evolución Diferencial}\\

  \vspace{0.7cm}
  \noindent{\textbf{Resumen}}\\

%  Las redes neuronales engloban un conjunto muy importante en el marco de lo que hoy se conoce por \textbf{Deep Learning}. Dentro de las mismas, estamos acostumbrados a asociar un optimizador por excelencia: \textbf{Gradiente Descendente}. En este proyecto se tratará de mostrar como existen otros algoritmos, con características que los hacen atractivos, que también pueden ser competitivos como optimizadores de redes neuronales: los algoritmos evolutivos. \\
%  
%  Más concretamente, se empleará la conocida por \textbf{Evolución Diferencial} auto-adaptativa (\textbf{SaDE}). Con el fin de mostrar esta competitividad, se tratará el problema de los dígitos manuscritos (MNIST) con distintos optimizadores: uno que es conocido que funciona muy bien (pues pertenece al conjunto de algoritmos que emplean la técnica de Gradiente Descendente) y es \textbf{RMSProp}; y una serie de algoritmos evolutivos que nos permitirán sacar ciertas conclusiones sobre cómo usarlos y derivarán en mostrar como hay uno, conocido por \textbf{Shade-ILS}, en que se hibridan la búsqueda local y la evolución diferencial (SHADE) y se consiguen resultados cercanos a los conseguidos por \textbf{RMSProp}, con lo que se obtiene la conclusión de que en efecto, los algoritmos evolutivos pueden ser competitivos como optimizadores de redes neuronales. \\
%  
\end{center}

 
En los últimos seis años, las redes neuronales profundas, comúnmente llamadas \textbf{Deep Learning}, han demostrado muy buenos resultados en diversos problemas de clasificación de imágenes, en algunos casos superando a personas expertas en sus respectivos campos. Una de las claves del éxito de estos modelos es el uso del \textbf{Gradiente Descendente Estocástico} o una de sus variantes, como es \textbf{RMSProp}, para optimizar el aprendizaje de estas redes. \\

Por otra parte, el conjunto de algoritmos evolutivos basados en \textbf{Evolución Diferencial}, han demostrado ser muy útiles para problemas de optimización continua. Sin embargo, todavía no está demostrado su potencial para el aprendizaje en redes neuronales. En este proyecto se analiza la capacidad de los algoritmos de \textbf{Evolución Diferencial auto-adaptativa (SaDE)}, así como sus variantes \textbf{L-SHADE} y \textbf{SHADE-ILS}, para el entrenamiento de las redes neuronales. Como caso de estudio se considera el problema popular de reconocimiento de dígitos manuscritos en imágenes (MNIST). Nuestros resultados muestran que la  \textbf{Evolución Diferencial} puede ser muy competitva con el \textbf{Gradiente Descendente}, a pesar de los muy buenos resultados obtenidos por el mismo,  como optimizador de redes neuronales.

\newpage
\end{document}
