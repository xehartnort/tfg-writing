\documentclass[../proyecto.tex]{memoir}

\begin{document}

\chapter{Metodología}

Antes de continuar con la implementación y exposición de la técnica de simulación Monte Carlo es necesario introducir un marco teórico matemático que nos servirá de referencia además de establecer una base de trabajo concisa. Los formalismos nos permitirán articular la intuición de asincronismo en el esquema de actualización de autómata celular, cuyo homónimo biológico sería el procesamiento imperfecto de información entre individuos a causa de las perturbaciones, derivadas del medio o de la interacción de los individuos. En este trabajo nos restringimos a un caso simple de asincronismo en la actualización: examinaremos que ocurre si todas las transiciones ocurren al mismo tiempo pero los individuos reciben la información del estado de sus vecinos de forma imperfecta.

\section{Marco matemático}

\subsection{Teoría de conjuntos}

\subsubsection{Introducción}

Nos gustaría poder plasmar la intuición de que una célula tenga la misma probabilidad de ser actualizada independientemente del resto de células actualizadas en cada instante de tiempo, por ello introducimos el siguiente concepto. 

\subsubsection{Ultrafiltros}
Dado un conjunto X, $P(X)$ denota el conjunto de todos los subconjuntos de X. Dado $A \in P(X)$, notaremos su complementario $A^{c}$. 

Un ultrafiltro de X es $U \in P(X)$ tal que:

\begin{enumerate}
\item $\emptyset \in U$.
\item Sean $A,B \in P(X)$ tales que $A \subset B$,$ A \in U$, entonces $B \in U$.
\item Si $A,B \in U$, entonces $A \cap B \in U$.
\item Si $A \in P(X)$ entonces $A \in U$ o $ A^{c} \in U$.
\end{enumerate}

Además dado $p \in X$, el ultrafiltro $U_{p}$ diremos que es principal si es el más pequeño que contiene a $p$, esto es, la colección de todos los conjuntos que contienen a $p$.

\subsection{Métodos de Monte Carlo}

\subsubsection{Introducción}

El nombre \textit{Monte Carlo} fue acuñado por los científicos que trabajaban en el desarrollo de armas nucleares en Los Álamos en la década de los 40 para designar una clase de métodos numéricos basados en el uso de números aleatorios. La esencia reside en la invención de juegos de azar cuyo comportamiento puede ser usado para estudiar algún fenómeno de interés. Se podría pensar que el hecho de que resultados obtenidos por estos métodos estén sujetos a las leyes del azar es un problema, sin embargo, es un problema menor, puesto que se puede determinar como de exactos son sus resultados y si se deseara obtener resultados más precisos, bastaría con incrementar el número de experimentos realizados. Actualmente, los métodos de Monte Carlos juegan un papel fundamental en la resolución de problemas matemáticos complejos, en los cuales, o bien los métodos de resolución analíticos o bien los métodos numéricos existentes requieren de grandes cantidades de memoria y tiempo cómputo.

En primer lugar introduciremos los conceptos básicos de teoría de la probabilidad sobre los que desarrollaremos las estimaciones \textit{Monte Carlo}, para dar paso al teorema central del límite, el cual, nos permitirá obtener estimaciones de la incertidumbre de nuestros cálculos. A continuación, discutiremos la elección de un generador de números aleatorios y procederemos con la metodología de las simulaciones realizadas.

\subsection{Teoría de la probabilidad}

% introducir concepto de simulación o experimento
 
Las deficiones de conceptos básicos de teoría de la probabilidad están extraídos de \cite{proman}. 

% también de http://mathworld.wolfram.com/RandomVariable.html

Una $\sigma$-álgebra sobre un conjunto X, es una colección no vacía de subconjuntos de X cerrados para uniones numerables y para la operación de complementario, esto es:
\begin{itemize}
\item $\forall A \in X$ se verifica que $A^{c} \in X$.
\item $ \forall A_{n} \in X, n \in \mathds{N} $ se verifica que $\bigcup _{n \in \mathds{N}} A_{n} \in X$.
\end{itemize}
 
Una $\sigma$-álgebra de Borel sobre X es la más pequeña $\sigma$-álgebra que contiene todos los conjuntos de $P(X)$. 
 
Sea $\mathds{B}$ una $\sigma$-álgebra de Borel de X, una medida de probabilidad, función de probabilidad o simplemente probabilidad es una función $\mu: \mathds{B} \rightarrow [0,1] $ tal que: 

Si $E_{1}, E_{2},... \subset \mathds{B}$ es una sucesión de conjuntos disjuntos, entonces:
\begin{equation*}
 \mu \big( \bigcup _{i=1} ^{\infty} E_{i} \big) = \sum _{i=1}^{\infty} \mu ( E_{i} ).
\end{equation*}

De esta manera, la probabilidad de un suceso $E \in \mathds{B}$ es $\mu(E)$.

Un espacio medible es la dupla (X,$\mathds{B}$) donde $\mathds{B}$ es la $\sigma$-álgebra de Borel sobre X, una función medible es una función entre espacios medibles, $g:  (X,\mathds{B}) \rightarrow (X',\mathds{B}')$ tal que: $g^{-1}(B) \in \mathds{B} \quad \forall B \in \mathds{B}'$.

Sea X un conjunto, $\mathds{B}$ la $\sigma$-álgebra de Borel sobre X, $P: \mathds{B} \rightarrow [0,1] $ una medida de probabilidad, la tupla (X,$\mathds{B}$, P) es un espacio de probabilidad. 

\subsubsection{Variables aleatorias}

% va a terminar siendo necesario hablar de las continuas y de las discontinuas, crack

Una variable aleatoria definida sobre un espacio de probabilidad $(X, \mathds{B}, P)$ es una función medible A: $(X, \mathds{B}, P) \rightarrow  (X', \mathds{B}')$ con $(X', \mathds{B}')$ un espacio medible.

Cada valor de A se corresponde con un subconjunto de puntos de $X$ que se aplica en dicho valor, es decir, $ \{ w\in X : A(w)=x\}$ que notaremos por simplicidad $\{X = x\}$. A parte de los anteriores conjuntos también nos resultarán de interés lo siguientes:
\begin{align*}
\{ w\in X : A(w) \leq x\} = \{ A \leq x \} \\
\{ w\in X : A(w) < x\} = \{ A < x \} \\
\{ w\in X : A(w) > x\} = \{ A > x \} \\
\{ w\in X : A(w) \geq x\} = \{ A \geq x \}
\end{align*}
Con esta útil notación podremos calcular la probabilidad de sucesos de mayor interés.

A partir, de ahora cuando nos refiramos a una variable aleatoria entenderemos que el codominio de dicha función medible es el espacio de medida $(\mathds{R},\mathds{B})$ y supondremos que está fijado un espacio de probabilidad $(X, \mathds{B}, P)$.


Dada una variable aleatoria $A$ se define su función de distribución como $ F : \mathds{R} \rightarrow [0,1] $ dada por $F(x)=P(A \leq x)$.

La función de distribución de $A$ satisface:

\begin{itemize}
\item Es monótona no decreciente.
\item Dada una sucesión decreciente de elementos de X, $\{x_n\}_{n \in \mathds{N}} \in X$, convergente a $x\in X$ se tiene $\lim_{x_n \searrow x} F(x_n) = F(x)$, es decir, es continua a la derecha. 
\item $\lim_{x\to+\infty} F(x) = 1$ y $\lim_{x\to-\infty} F(x) = 0$.
\end{itemize}

\subsubsection{Variables aleatorias discretas}

Una variable aleatoria $A$ diremos que es discreta si toma valores es un conjunto numerable, esto es, $\exists E \subset \mathds{R}$ tal que $P(A \in E)=1$. Su función de distribución es la siguiente: 

\begin{align*}
\forall x\in \mathds{R}, \quad F(x) = \sum_{x_n\in E, x_n \leq x} P(A=x_n).
\end{align*}

Definimos el enésimo momento de la variable aleatoria discreta A como:

\begin{align*}
\mathds{E}(A^n) \equiv \sum_{x_m \in E, x_m \leq x} x_m^n P(A=x_m).
\end{align*}

Notar que por definición el momento n=0 siempre existe, sin embargo el resto de momentos no existen necesariamente, es decir, pueden divergir. Por otro lado, el momento n=1 se conoce como valor esperado o esperanza.

Suponiendo que los momentos de órdenes 1 y 2 existen, definimos la varianza de la variable aleatoria discreta A como:

\begin{align*}
var(A) \equiv \sum_{x_m\in E, x_m \leq x} (P(A=x_m) - \mathds{E}(A))^2.
\end{align*}

A la raíz cuadrada positiva de la varianza la notaremos $\sigma(A)=+\sqrt{var(A)}$ y diremos que es la desviación estándar de la variable aleatoria discreta A. Dicho valor es una medida de dispersión de A, de esta manera si A es constante, $A = c\in \mathds{R}$ se tiene $\mathds{E}(A)=c$ y $var(A) = 0$.

\subsubsection{Variables aleatorias continuas}

% hablar de variable aleatoria continua

% introducir esperanza

% momentos

% y varianza :d



\subsubsection{Tipos de convergencia: convergencia en probabilidad y convergencia casi segura}

Sean ${A_n}$ ,$n\in \mathds{N}$ y $A$ variables aleatorias, definimos:

\begin{itemize}

\item $A_n \to A$ en probabilidad, si para todo $\epsilon > 0$, $\lim_{n\to\infty} P( |A_n-A|> \epsilon ) = 0$ y lo notaremos $A_n \to^{P} A$.

\item $A_n \to A$ casi seguramente, si $P(\lim_{n\to\infty} A_n=A) = 1$ y lo notaremos $A_n \to^{c.s.} A$.

\item $A_n \to A$ en distribución, si $\lim_{n \to \infty} P(A_n \leq x) = P(A \leq x),\quad \forall x \in \mathds{R}$ donde $x\mapsto P(A \leq x)$ es una función continua.

\end{itemize}


%% Todo lo que haga falta para terminar demostrando el teorema de la Ley de los Grandes números + la desigualdad de tchebyshev. Esto es:

%% algo de teoría de estimadores, aunque no sé muy bien para qué

%% CONCEPTO DE INDEPENDENCIA


\subsubsection{Teorema central del límite}

Previa a la demostración del teorema central del límite, introducimos las herramientas matemáticas que nos harán posible su demostración.

Teorema de Taylor

Sea $k \in \mathds{N}$ y $f: \mathds{R} \to \mathds{R}$ k veces diferenciable en el punto $a \in \mathds{R}$. Entonces existe una función $h_k: \mathds{R} \to \mathds{R}$ tal que:

$$
f(x)=f(a)+f'(a)(x-a)+\frac{f''(a)}{2!}(x-a)^2+\dotsb+\frac{f^{(k)}(a)}{k!}(x-a)^k + h_k(x)(x-a)^k,
$$

y $\lim_{x\to a} h_k(x) = 0$.
 
\subsubsection{Función generatriz de momentos}

Sea A una variable aleatoria con todos los momentos de orden $n\in\mathds{N}$ finitos, se define su función generatriz de momentos como:

$$
M_A(t)=\mathds{E}[e^{tA}].
$$

Expandiendo la serie de Taylor de $e^{tA}$, entendemos por qué se le llama función generatriz de momentos:

$$
M_A(t)=\sum_{n=0}^{\infty} \frac{\mathds{E}(A^n)}{n!}t^n.
$$

En particular, podemos observar su relación con los momentos de orden n:

$$
M_X(0)=\mathds{E}[A^n]
$$

Sean A y A' dos variables aleatorias independientes y $c \in \mathds{R}$, tenemos que la función generatriz de momentos verifica lo siguiente:

$$
M_{A+A'}(t)=\mathds{E}[e^{t(A+A')}]=\mathds{E}[e^{tA}e^{tA'}] = M_A(t)M_B(t),
$$

$$
M_{cA}(t)=\mathds{E}[e^{tcX}]=M_A(ct).
$$

A continuación vamos exponer como se puede obtener relajar la condición de existencia de momentos

%También tenemos que introducir lo que es una función característica :DDDD
% y quizás no estaría mal decir algo de la exponencial compleja :D

% Aquí también introducimos el teorema de Levy para usar la demostración que usa funciones características

\subsubsection{Ley de los grandes números, formulación fuerte}

Sean $A_n$, $n \in \mathds{N}$ variables aleatorias independientes e idénticamente distribuidas, es decir, obtenidas de la misma distribución con esperanza (finita) $\mu$. Entonces el valor medio de $A_n$, $\bar{\mu}$, converge casi seguramente a $\mu$, $\bar{\mu}=\frac{1}{n}\sum_{n\in\mathds{N}} A_n \to^{c.s.} \mu$, esto es, $P(\lim_{n\to\infty} \bar{\mu}=\mu) = 1$.

Para obtener una demostración un tanto más breve de éste importante teorema añadiremos la hipótesis de existencia del momento de orden 4 de $A_n$. Una demostración completa del teorema sin la hipótesis adicional se puede consultar en [].

Lema extra, previo a la demostración:
En las misma condiciones de la ley de los grandes números, existe una constante $ K < \infty $ tal que para todo $ n \geq 0$:
$$
	\mathds{E} \big( ( \bar{ \mu } - n \mu ) ^ 4 \big) \leq K n^2.
$$

Demostración:

Sean $Z_k = A_k - \mu $ y $T_n = Z_1 + Z_2 + ... + Z_n = \sum_{i=1}^{n} A_n - n\mu $. Entonces:

$$
	\mathds{E} ( T_{n}^{4} ) = \mathds{E} ( \big( \sum_{i=1}^{n} Z_i \big) ^{4} ) = n\mathds{E}(Z_{1}^4)+3n(n-1)\mathds{E}(Z_1^2 Z_2^2) \leq Kn^2.
$$

donde en la segunda igualdad se ha empleado el desarrollo multinomial:

$$
(x_1+x_2+...+x_m)^n = \sum_{k_1+k_2+...+k_m=n} { n \choose k_1,k_2, ..., k_n} \prod_{1 \leq t \leq m} x_t^{k_t}
$$

con

$$
{ n \choose k_1,k_2, ..., k_n} = \frac{n!}{k_1!k_2! \dotsb k_n!}
$$

Considerando que claramente $\mathds{E}(Z_k)=0 \quad \forall k$ y la independencia de las variables $Z_k$, se han cancelado todos los sumandos de la forma:

\begin{align*}
	\mathds{E} (Z_{i} Z_{j}^3 ) =\mathds{E} (Z_{i}) \mathds{E} (Z_{j}^3) = 0, \quad 1 \leq i,j \leq n, \quad i \neq j, \\
	\mathds{E} ( Z_{i} Z_{j} Z_{k} Z_{l} ) = \mathds{E} (Z_{i}) \mathds{E} (Z_{j}) \mathds{E} (Z_{k}) \mathds{E} (Z_{l}) = 0, \quad 1 \leq i,j,k,l \leq n, \quad i \neq j \neq k \neq l.
\end{align*}

y $K$ adecuadamente elegida, $K = 4 max{\mathds{E}(Z_1^4), \mathds{E}(Z_1^2)^2}$.

\subsubsection{Demostración de la ley de los grandes números}

Asumamos que $E(A_n^4) < \infty \forall n$, aplicando el lema anterior:

$$
	\mathds{E} \big( ( \bar{ \mu } - \mu ) ^ 4 \big) \leq K n^{-2}.
$$

Ahora por el lema de Tonelli:

$$
	\mathds{E} \big( \sum_{n \geq 1}( \bar{ \mu } - \mu ) ^ 4 \big) = \sum_{n \geq 1} \mathds{E} \big( ( \bar{ \mu } - \mu ) ^ 4 \big)  < \infty
$$

lo que implica:

$$
\sum_{n \geq 1} \big( \bar{ \mu } - \mu \big)^ 4 < \infty \quad c.s. 
$$

Pero si una serie es convergente, entonces la sucesión de su término general de la serie converge a cero, por tanto:
$$
 \bar{ \mu } \to \mu \quad c.s.
$$
% 

\subsection{•}


\subsection{Generadores de números aleatorios}

\subsubsection{Introducción}

Curiosamente, en las definiciones de los métodos de Monte Carlo no hay referencia explícita al empleo de la capacidades de cómputo de los ordenadores, sin embargo el gran desarrollo que han experimentado éstos desde el último tercio de siglo XX hasta nuestros días, los ha convertido en herramientas indispensables en las simulaciones Monte Carlo. La generación de números aleatorios ha experimentado también un importante crecimiento en las últimas décadas. En un principio los generadores de números aleatorios  más usados venían expresados por la siguiente ecuación recurrente:

\begin{equation} \label{cong}
I_{j+1} = aI_{j} +c \mod (m)
\end{equation}

donde $a$ es un entero positivo llamado multiplicador y $c$ es un número natural llamado incremento. Para $c \neq 0$, \ref{cong} es conocido por el nombre: generador lineal congruente de números aleatorios. Claramente, en $n<m$ pasos la ecuación \ref{cong} comienza a generar valores duplicados en el mismo orden, conocido ésto, se hacían elecciones particulares de a,c y m que obtuvieran en mayor periodo posible. Notar que la elección del valor inicial $I_{0}$ no es relevante, pues se generarán todos los naturales posibles entre $0$ y $m-1$ antes de la primera repetición. Una de las primeras debilidades que se encontraron en este tipo de generadores es que si $n$ números aleatorios se utilizan para pintar puntos en el espacio $[0,1]^{n}$, los puntos no tenderán a "rellenar" el espacio si no que se agruparán en planos de dimensión $n-1$ \cite{planos}. 

Éste desarrollo no solo se ha plasmado en el desarrollo de nuevas técnicas de generación de números aleatorios, si no que también se han desarrollado baterías de test estadísticos para comprobar la eficacia de dichos generadores. Emplearemos la batería de test \textit{Dieharder}, la cual está basada en los primeros test estadísticos propuestos por George Marsaglia en \textit{Diehard battery of tests} e incluye también los test desarrollados por el NIST (National Institute for Standards and Technology) \cite{dieharder}.

\subsubsection{Discusión}



\subsection{Teoría de la computación}

\subsubsection{Introducción}
Dado que el comportamiento completamente síncrono de un autómata celular como herramienta de modelado es una rareza, se han realizado numerosas investigaciones empíricas del autómata celular asíncrono. Sin embargo, los pocos análisis formales realizados o bien se refieren a ejemplos o a casos particulares de asíncronicidad. [Alguna citilla mona]. Así pues tomaremos el concepto más generalista de autómata celular m-asíncrono \cite{oraculo}, cuya idea principal es tener algún tipo de oráculo el cual en cada unidad discreta de tiempo dice las células que tienen que ser actualizadas. Dicho oráculo se implementa empleando una medida de probabilidad $\mu$ sobre subconjuntos de enteros d-dimensionales, $\mathds{Z}^{d}$. Notar que la definición con la que trataremos es la extensión a espacios multidimensionales de la dada en \cite{oraculo}.

\subsubsection{Autómata celular determinista}
Un autómata celular determinista es un sistema dinámico discreto consistente en un array $d$-dimensional de autómatas finitos, llamados células. Cada célula está conectada uniformemente a un vecindario formado por un número finito de células, y tiene un estado de un conjunto finito de estados. Actualiza su estado de acuerdo a una función de transición la cual determina el estado de una célula considerando su propio estado y el de su vecindario. 

Formalmente, la tupla $A=(\mathds{Z}^{d}, N, Q, f)$ es un autómata celular determinista, de ahora en adelante autómata celular, donde $\mathds{Z} ^{d}$ es un espacio de células $d$-dimensional, $Q$ el conjunto de estados posibles para cada célula y $N \in (\mathds{Z}^{d})^{k}$ el vecindario genérico de un autómata celular, esto es, para $N=(n_{1},...,n_{k})$, $a \in \mathds{Z} ^{d}$ célula, cada célula en $\{(a+n_{1},...,a+n_{k})\}$ es una célula vecina de $a$ y $f:Q^{k+1} \rightarrow Q$ es la función de transición local que define la transición de estado de cada célula como función de su propio estado y del estado de cada célula en su vecindario. Una configuración es una función $g: \mathds{Z}^{d} \rightarrow Q$, la cual a cada punto del espacio $\mathds{Z}^{d}$ le asigna un estado del conjunto de estados $Q$, al conjunto de las configuración lo notaremos $Q^{\mathds{Z}^{d}}$. La función de transición local, fijando una configuración g induce una función de transición global $F:Q^{\mathds{Z}^{d}} \rightarrow Q^{\mathds{Z}^{d}}$ definida como sigue:

\begin{equation*}
\forall x \in Q^{\mathds{Z}^{d}}, \quad \forall i \in \mathds{Z}^{d}, \quad F(x)(i) = f(x(i),x(i+n_{1}),...,x(i+n_{k}))
\end{equation*}

\subsection{Autómata celular m-asíncrono}

Un autómata celular m-asíncrono C es la tupla $(A, \mu)$ donde A es un autómata celular y $\mu$ es una medida de probabilidad sobre la $\sigma$-álgebra de Borel en $P(\mathds{Z}^{d})$. 
Para cada función de transición local $f$ y cada conjunto $\tau \in P(\mathds{Z}^{d})$, definimos la función de transición global $F:Q^{\mathds{Z}^{d}} \rightarrow Q^{\mathds{Z}^{d}}$ como sigue:

\begin{equation*}
	\forall x \in Q^{\mathds{Z}^{d}}, \quad \forall i \in \mathds{Z}^{d}, \qquad
	F_{\tau}(x)(i) = \left\{ \begin{array}{lcc}
             f(x(i),x(i+n_{1}),...,x(i+n_{k})) &   si  & i \in \tau ,\\
             \\ x(i) & si  & i \notin \tau .\\
             \end{array}
             \right.
\end{equation*}

$F_{\tau}$ aplica la función de transición local solo sobre los elementos de $\tau \subset \mathds{Z}^{d}$. Notar que cada célula $i \in \mathds{Z}^{d}$ es actualizada con probabilidad $\mu(U_{i})$.

Esta nueva definición de autómata celular m-asíncrono, incluye la de autómata celular síncrono. Fijada una $\sigma$-álgebra $\mathds{B}$ sobre $\mathds{Z}^{d}$ y sea $C_{0}=(A, \mu_{0})$ un autómata celular m-asíncrono donde $\mu_{0}: \mathds{B} \rightarrow [0,1]$ viene dada por: 

\begin{equation*}
	 \forall A \in P(\mathds{Z}^{d}), \qquad 
	 \mu_{0}(A) = \left\{ \begin{array}{lcc}
             \ 1 &   si  & \mathds{Z}^{d} \in A ,\\
             \\0 &   si  & \mathds{Z}^{d} \notin A .\\
             \end{array}
             \right.
\end{equation*}

De ésta manera, $\mu_{0}(\{\mathds{Z}^{d}\})=1$ y por lo tanto, en cada instante de tiempo se aplicará la función de transición local sobre $\mathds{Z}^{d}$.

Por otro lado, también contiene el concepto de evolución totalmente asíncrona comentado en la introducción, esto es, en cada instante de tiempo solo de aplica la función de transición local a una sola célula. Consideramos ahora el autómata celular m-asíncrono $C_{1}=(A, \mu_{1})$ donde $\mu_{1}: \mathds{B} \rightarrow [0,1]$ verifica lo siguiente:

\begin{enumerate}
\item $\mu_{1}(U_{i}) > 0, \quad \forall i \in \mathds{Z}^{d}$.
\item $\mu_{1}(U_{i} \cap U_{j}) = 0, \quad \forall i \neq j, \quad i,j \in \mathds{Z}^{d}$.
\end{enumerate}

Así solo los conjuntos de la forma \{k\} $(k \in \mathds{Z}^{d})$ se les aplica la función de transición local.

Por último, contiene el concepto de evolución $\alpha$-asíncrona que nos interesa. Dado $C_{2}=(A, \mu_{2})$ un autómata celular m-asíncrono y sea $\alpha \in (0,1)$ la probabilidad con la que se actualizan las células, $\mu_{2}: \mathds{B} \rightarrow [0,1]$ satisfaciendo:

\begin{enumerate}
\item $\mu_{2}(U_{i}) = \alpha, \quad \forall i \in \mathds{Z}^{d}$.
\item $ \forall A \subseteq \mathds{Z}^{d} \quad$ finito,$\quad  \mu_{2} ( \bigcap_{a \in A} U_{a} ) = \prod_{a \in A} \mu_{2} ( U_{a} )$.
\end{enumerate}


\subsection{Juego de vida de Conway}

El juego de vida de Conway es un autómata celular síncrono:

\begin{equation}
C = (\mathds{Z}^{2} , N=\{(-1, 1), (0, 1), (1, 1), (-1, 0), (1, 0), (-1,-1), (0,-1), (1,-1) \}, Q=\{0,1\}, f)
\end{equation}
 
donde $f:\{0,1\}^{9} \rightarrow \{0,1\} $ viene dada por:

\begin{equation}
f(x)= \left\{ \begin{array}{lcc}
             1 &   si  & x_{0}=0 \quad y \quad \sum_{i=1}^{8} x_i = 3 \\
             \\ 1 & si & x_{0}=1 \quad y \quad \sum_{i=1}^{8} x_i \in \{2 ,3\} \\
             \\ 0 &  si  & \sum_{i=1}^{8} x_i \notin \{2, 3\} \
             \end{array}
   \right. 
\end{equation}
y $x = (x_{0}, x_{1}, ...,x_{8}) = (c,c+n_{1},...,c+n_{8})$ con $c \in \mathds{Z} ^{d}$ célula.


\subsection{Juego de vida de Conway asíncrono}


\end{document}
