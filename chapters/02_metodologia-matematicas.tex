\documentclass[../proyecto.tex]{memoir}

\begin{document}

\chapter{Metodología}

Antes de continuar con la implementación y exposición de la técnica de simulación Monte Carlo es necesario introducir un marco teórico matemático que nos servirá de referencia además de establecer una base de trabajo concisa. Los formalismos nos permitirán articular la intuición de asincronismo en el esquema de actualización de autómata celular, cuyo homónimo biológico sería el procesamiento imperfecto de información entre individuos a causa de las perturbaciones, derivadas del medio o de la interacción de los individuos. En este trabajo nos restringimos a un caso simple de asincronismo en la actualización: examinaremos que ocurre si todas las transiciones ocurren al mismo tiempo pero los individuos reciben la información del estado de sus vecinos de forma imperfecta.

\section{Introducción}

El nombre \textit{Monte Carlo} fue acuñado por los científicos que trabajaban en el desarrollo de armas nucleares en Los Álamos en la década de los 40 para designar una clase de métodos numéricos basados en el uso de números aleatorios. La esencia reside en la invención de juegos de azar cuyo comportamiento puede ser usado para estudiar algún fenómeno de interés. Se podría pensar que el hecho de que resultados obtenidos por estos métodos estén sujetos a las leyes del azar es un problema, sin embargo, es un problema menor, puesto que se puede determinar como de exactos son sus resultados y si se deseara obtener resultados más precisos, bastaría con incrementar el número de experimentos realizados. Actualmente, los métodos de Monte Carlos juegan un papel fundamental en la resolución de problemas matemáticos complejos, en los cuales, o bien los métodos de resolución analíticos o bien los métodos numéricos existentes requieren de grandes cantidades de memoria y tiempo cómputo.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Extender un poco y reescribir esto cuando se haya terminado la sección %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

En primer lugar introduciremos los conceptos básicos de teoría de la probabilidad sobre los que desarrollaremos las estimaciones \textit{Monte Carlo}, para dar paso al teorema central del límite, el cual, nos permitirá obtener estimaciones de la incertidumbre de nuestros cálculos. A continuación, discutiremos la elección de un generador de números aleatorios y procederemos con la metodología de las simulaciones realizadas.

\section{Teoría de conjuntos}

\subsection{Introducción}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% darle una vuelta, en realidad se introduce esto para obtener una definición más general de autómata y permitir que la actualización dependa o no del resto %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Nos gustaría poder plasmar la intuición de que una célula tenga la misma probabilidad de ser actualizada independientemente del resto de células actualizadas en cada instante de tiempo, por ello introducimos la definición de ultrafiltro. 

Dado un conjunto X, $P(X)$ denota el conjunto de todos los subconjuntos de X. Dado $A \in P(X)$, notaremos su complementario $A^{c}$. 
\begin{defi}
Un ultrafiltro de X es $U \in P(X)$ tal que:

\begin{enumerate}
\item $\emptyset \in U$.
\item Sean $A,B \in P(X)$ tales que $A \subset B$,$ A \in U$, entonces $B \in U$.
\item Si $A,B \in U$, entonces $A \cap B \in U$.
\item Si $A \in P(X)$ entonces $A \in U$ o $ A^{c} \in U$.
\end{enumerate}

Además dado $p \in X$, el ultrafiltro $U_{p}$ diremos que es principal si es el más pequeño que contiene a $p$, esto es, la colección de todos los conjuntos que contienen a $p$.
\end{defi}

\section{Teoría de la probabilidad: conceptos básicos}

El contenido de esta sección está extraído de los siguientes libros en orden de empleo: \cite{elLibro}, \cite{loeve}, \cite{}. 

\begin{defi}
Una $\sigma$-álgebra, $\mathds{F}$, sobre un conjunto X, es una colección no vacía de subconjuntos de X cerrados para uniones numerables y para la operación de complementario, esto es:
\begin{itemize}
\item $\forall A \in \mathds{F}$ se verifica que $A^{c} \in \mathds{F}$.
\item $ \forall A_{n} \in \mathds{F}, n \in \mathds{N} $ se verifica que $\bigcup _{n \in \mathds{N}} A_{n} \in \mathds{F}$.
\end{itemize}
\end{defi}

\begin{defi}
Sean un conjunto X con su $\sigma$-álgebra asociada, $\mathds{F}$, el par $(X, \mathds{F})$ es un espacio medible. 
\end{defi}

\begin{defi}
Una función medible es una función entre espacios medibles, $g:  (X,\mathds{F}) \rightarrow (X',\mathds{F}')$ tal que: $g^{-1}(A) \in \mathds{F} \quad \forall A \in \mathds{F}'$.
\end{defi}

\begin{defi}
La tupla $(X, \mathds{F}, P)$ es un espacio de probabilidad si:
\begin{itemize}
\item $X$ es el espacio de muestreo, esto es, algún conjunto,
\item $\mathds{F}$ es una $\sigma$-álgebra de sucesos, esto es, los conjuntos medibles de $X$,
\item P es una medida de probabilidad, 
esto es, P satisface los siguientes axiomas de Kolmogorov:
\begin{enumerate}
\item Para cada $A\in\mathds{F}$, existe un número $P(A) \geq 0$, esto es, la probabilidad del suceso A,
\item $P(X)=1$.
\item Sean ${A_n, n \geq 1}$ disjuntos, entonces: $$
	P \left( \bigcup_{n=1}^{\infty} A_{n} \right) = \sum_{n=1}^{\infty} P(A_n).
$$
\end{enumerate}
\end{itemize}
\end{defi}

\begin{defi}
Los sucesos ${A_n, n \geq 1}$ son independientes si y solo si $$
P \left( \bigcap_{n \geq 1} A_{n} \right) = \prod_{n \geq 1} P(A_n).
$$
\end{defi}

De ahora en adelante trabajaremos fijando $X=\mathds{R}$ y $\mathds{F}=\mathds{B}$ la $\sigma$-álgebra de Borel la recta real.
\begin{defi}
Un conjunto A es abierto si, para cada punto $x\in A$, existe una bola de centro el punto y radio $\epsilon > 0$ tal que $B(x,\epsilon)=\{ z : \abs{z-x} < \epsilon\} \subset F$. Así la $\sigma$-álgebra de Borel , es aquella generada por los conjuntos abiertos de $\mathds{R}$.
\end{defi}

\subsection{Variables aleatorias}

[MIRAR AQUÍ bien]
\begin{defi}
Una variable aleatoria definida sobre un espacio de probabilidad $(X, \mathds{B}, P)$ es una función medible A: $X \rightarrow \mathds{R}$
\end{defi}

Cada valor de A se corresponde con un subconjunto de puntos de $X$ que se aplica en dicho valor, es decir, $ \{ w\in X : A(w)=x\}$ que notaremos por simplicidad $\{X = x\}$. A parte de los anteriores conjuntos también nos resultarán de interés lo siguientes:
\begin{align*}
\{ w\in X : A(w) \leq x\} = \{ A \leq x \} \\
\{ w\in X : A(w) < x\} = \{ A < x \} \\
\{ w\in X : A(w) > x\} = \{ A > x \} \\
\{ w\in X : A(w) \geq x\} = \{ A \geq x \}
\end{align*}
Con esta útil notación podremos notar fácilmente la probabilidad de sucesos de mayor interés.

\begin{prop} \label{funcion_de_va}
Si g es una función real y medible y A es una variable aleatoria entonces A'=g(X) es una variable aleatoria.
\end{prop}

\begin{defi}
Dada una variable aleatoria $A$ se define su función de distribución como $ F : \mathds{R} \rightarrow [0,1] $ dada por 
$$
F(x)=P(A \leq x).
$$
\end{defi}

\begin{prop}
La función de distribución de la variable aleatoria $A$ satisface:

\begin{itemize}
\item Es monótona no decreciente.
\item Dada una sucesión decreciente de elementos de X, $\{x_n\}_{n \in \mathds{N}} \in X$, convergente a $x\in X$ se tiene $\lim_{x_n \searrow x} F(x_n) = F(x)$, es decir, es continua a la derecha. 
\item $\lim_{x\to+\infty} F(x) = 1$ y $\lim_{x\to-\infty} F(x) = 0$.
\end{itemize}
\end{prop}

\begin{defi}
Sea A una variable aleatoria, definimos su valor esperado o esperanza como sigue:
$$
E(A) = \int_{X}A(w)dP(w) = \int AdP.
$$
Adicionalmente si $E\abs{A} < \infty$, diremos que $A$ es integrable.
\end{defi}

\begin{prop}
Sean $A$ y $A'$ dos variables aleatorias integrables entonces: $$
E(aA+bA')=aE(A)+bE(A'), \quad \forall a,b \in \mathds{R}.
$$
\end{prop}

\begin{defi}
Sea una variable aleatoria A, definimos:
\begin{itemize}
\item Los momentos de A: $\mathds{E}(A^n) = \int_{X}A(w)^{n}dP(w), \quad n \in \mathds{N}$.
\item Los momentos centrados de A: $\mathds{E}_c(A^n) = E( (A-E(A))^n ), \quad n \in \mathds{N}$.
\end{itemize}
\end{defi}

Notar que los momentos no existen necesariamente para todo $n$.

\begin{defi}
Definimos la varianza de la variable aleatoria A con esperanza $\mu < \infty$ y $\mathds{E}_c(A^2) < \infty $ como:$$
var(A) \equiv \mathds{E}_c(A^2) = \mathds{E}(A^2) - \mu^2.
$$
\end{defi}

\begin{defi}
A la raíz cuadrada positiva de la varianza la notaremos $\sigma(A)=+\sqrt{var(A)}$ y diremos que es la desviación estándar de la variable aleatoria A. 
\end{defi}

\begin{defi}
La variables aleatorias $A_1, A_2,..., A_n$ son independientes si y solo si, para arbitrarios conjuntos de la $\sigma$-álgebra de Borel $B_1, B_2,..., B_n$: $$
	P \left( \bigcap_{k = 1}^{n} \{X_k \in A_k\} \right) = \prod_{k = 1}^{n} P(X_k \in A_k).
$$
\end{defi}

\subsection{Variables aleatorias discretas}

\begin{defi}
Una variable aleatoria $A$ diremos que es discreta si toma valores es un conjunto numerable, esto es, $\exists E=\{x_n\}_{n \in \mathds{N}} \subset \mathds{R}$ tal que $P(A \in E)=1$. 
\end{defi}

\begin{defi}
La función de distribución de probabilidad de una variable aleatoria $A$ es la siguiente: $$
\forall x\in \mathds{R}, \quad F(x) = P( A \leq x) = \sum_{x_n\in E, x_n \leq x} P(A=x_n).
$$
\end{defi}

\begin{defi}
Sea una variable aleatoria discreta A, definimos:

\begin{itemize}
\item Los momentos de A: $\mathds{E}(A^n) = \sum_{x_m \in E} x_m^n P(A=x_m)$.
\item Los momentos centrados de A: $\mathds{E}_c(A^n) =\sum_{x_m \in E} (x_m - \mu)^n P(A=x_m)$.
\end{itemize}
\end{defi}

El momento n=1 se conoce como valor esperado o esperanza. $$
\mathds{E}(A) \equiv \sum_{x_m \in E} x_m P(A=x_m).
$$

\begin{defi}[Tipos de convergencia: convergencia en probabilidad y convergencia casi segura]
Sean ${A_n}$ ,$n\in \mathds{N}$ y $A$ variables aleatorias, definimos:
\begin{itemize}
\item $A_n \to A$ en probabilidad, si para todo $\epsilon > 0$, $\lim_{n\to\infty} P( |A_n-A|> \epsilon ) = 0$ y lo notaremos $A_n \to^{P} A$.
\item $A_n \to A$ casi seguramente, si $P(\lim_{n\to\infty} A_n=A) = 1$ y lo notaremos $A_n \to^{c.s.} A$.
\item $A_n \to A$ en distribución, si $\lim_{n \to \infty} P(A_n \leq x) = P(A \leq x),\quad \forall x \in \mathds{R}$ donde $x\mapsto P(A \leq x)$ es una función continua y lo notaremos $A_n \to^{d} A$.
\end{itemize}
\end{defi}

\subsection{Distribución normal}

\begin{defi}
Diremos que una variable aleatoria A tiene una distribución normal con media $\mu < \infty$ u varianza $\sigma^2 < \infty$, $N(\mu,\sigma^2)$, si A tiene una distribución con la siguiente función de distribución de probabilidad: 

$$
f(x| \mu, \sigma^2) = \frac{1}{ \sigma \sqrt{ 2 \pi }} exp \left( -\frac{1}{2}\left( \frac{x-\mu}{\sigma} \right)^2 \right),\quad \forall x \in \mathds{R}.
$$

\end{defi}

\section{Teoría de la probabilidad: Teorema central del límite}

\begin{teorema}
Sean $A_{1},A_{2},...,A_{n}$ variables aleatorias independientes, con esperanza $\mu < \infty$, varianza $\sigma^2 < \infty$ e idénticamente distribuidas. Entonces $$
\frac{ \frac{1}{n}\sum_{i=1}^nA_i - n\mu}{ \sigma \sqrt{n}} \to^d N(0,1).
$$


\end{teorema}

Previa a la demostración del teorema central del límite, introducimos las herramientas matemáticas que nos harán posible su demostración.

\begin{teorema}[Teorema de Taylor]

Sea $k \in \mathds{N}$ y $f: \mathds{R} \to \mathds{R}$ k veces diferenciable en el punto $a \in \mathds{R}$. Entonces existe una función $h_k: \mathds{R} \to \mathds{R}$ tal que:

$$
f(x)=f(a)+f'(a)(x-a)+\frac{f''(a)}{2!}(x-a)^2+\dotsb+\frac{f^{(k)}(a)}{k!}(x-a)^k + h_k(x)(x-a)^k,
$$

y $\lim_{x\to a} h_k(x) = 0$.
\end{teorema}

\begin{defi}
Sea $i$ el número completo $i=\sqrt{-1}$, la extensión de la función exponencial $exp: \mathds{R} \to \mathds{R^{+}}$ real al cuerpo de los números complejos $exp: \mathds{C} \to \mathds{C}$ dada por:
$$
z \mapsto exp(iz) = e^{iz}=\cos z+i\sin z.
$$
\end{defi}

Notar que el módulo de la exponencial compleja está acotado por la unidad :

$$
\abs{ exp(iz) } = \abs{\cos z+i\sin z} = \sqrt{\cos^2 z - \sin^2 z} \leq \sqrt{\cos^2 z + \sin^2 z} = 1.
$$

Esta propiedad de la exponencial compleja no asegura la existencia de la siguiente definición para toda variable aleatoria.

\begin{defi}
La función característica asociada a la variable aleatoria A es la función $\phi_{A}: \mathds{R} \to \mathds{C}$, dada por:
$$
\phi_{A}(t) = \mathds{E}(e^{itA}).
$$
\end{defi}


\begin{prop}
Sea $\phi_A$ la función característica de la variable aleatoria A, entonces:

\begin{itemize}
\item $\abs{\phi_A(t)} \leq \phi_A(0)=1$.
\item $\phi_A$ es una función uniformemente continua.
\item $\phi_{cA+b}(t)=e^{itb}\phi_A(ct), \quad c,b\in \mathds{R}$.
\item Sea $A_1, A_2,...,A_n, n\in\mathds{N}$ una sucesión finita de variables aleatorias independientes, entonces: $$ 
\phi_{\sum^{n}_{i=1} A_i} (t) = \prod_{i=1}^{n} \phi_{A_i} (t) .
$$ Además si $A_1, A_2,...,A_n$ son idénticamente distribuidas: $$
\phi_{\sum^{n}_{i=1} A_i} (t) = \left( \phi_{X_1}(t) \right)^{n}.
$$
\item Si $\mathds{E}(A^k) < \infty$ su derivada k-ésima evaluada en 0 es $\phi_A^{(k)}(0)=i^k\mathds{E}(A^k)$.
\end{itemize}

\end{prop}

\begin{teorema}[Teorema de continuidad]
Sean $A_1, A_2,...,A_n, n\in\mathds{N}$ variables aleatorias entonces: $$
\lim_{n \to \infty }{\phi_{A_n}} = \phi_{A}(t), \quad \forall t\in \mathds{R},
$$
si y solo si $$
X_n \to^d X.
$$

\end{teorema}

\begin{proof}[Demostración (Teorema central del límite)]

Sean $A_{1},A_{2},...,A_{n}$ variables aleatorias independientes idénticamente distribuidas con esperanza $\mu < \infty$ y varianza $\sigma^2< \infty$.
Sea ahora $$
Z_{n} = \frac{A_{1}+A_{2}+...+A_{n}-n\mu }{\sigma \sqrt{n}}.
$$
Definimos una nueva variable aleatoria, $Y_i$, que es la versión normalizada de $A_i$:$$
Y_i=\frac{A_i-\mu}{\sigma}
$$
Así definida, $Y_i$ es idénticamente distribuida con esperanza y varianza: $$
E(Y_{i}) = 0,  Var(Y_{i})=1.
$$
Sea ahora $Z_n = \frac{Y_1+Y_2+...+Y_n}{\sqrt{n}}$, queremos ver que $$
\lim_{n \to \infty} \phi_{Z_{n}}(t)=e^{-\frac{t^{2}}{2}}
$$

Procedemos a desarrollar el

$$
\phi_{\frac{Y_1+Y_2+...+Y_n}{\sqrt{n}}}(t) = \prod_{i=1}^{n} \phi_{\frac{Y_i}{\sqrt{n}}} (t) = \big(\phi_{\frac{Y_1}{\sqrt{n}}}(t) \big)^n
$$

Aplicando el Teorema de Taylor para obtener el desarrollo centrado en 0 para $k=2$ de $\phi_{Y_1}(\frac{t}{\sqrt{n}})$: 

\begin{align*}
\phi_{\frac{Y_1}{\sqrt{n}}}(t) &= \phi_{Y_1}(\frac{t}{\sqrt{n}}) \\
  &= \phi_{Y_1}(0) + \frac{t}{\sqrt{n}}\phi_{Y_1}^{'}(0) + \frac{t^2}{2n}\phi_{Y_1}^{''}(0) + \frac{t^2}{n} h_2(t)\\
 &= 1 + i\frac{t}{\sqrt{n}}\mathds{E}(Y_1) - \frac{t^2}{2n}\mathds{E}(Y_1^2) + \frac{t^2}{n} h_2(t)\\
 &= 1 + 0 - \frac{t^2}{2n} + \frac{t^2}{n} h_2(\frac{t}{\sqrt{n}}),
\end{align*}
donde $\lim_{t\to 0} h_2(\frac{t}{\sqrt{n}}) = 0$.

Así obtenemos que $$
\big(\phi_{Y_1}(\frac{t}{\sqrt{n}}) \big)^n =\big( 1 - \frac{t^2}{2n} + \frac{t^2}{n} h_2(\frac{t}{\sqrt{n}}) \big)^n\longrightarrow e^{-\frac{t^2}{2}}.
$$

cuyo límite es la función característica de una variable perteneciente a una distribución normal con media 0 y varianza 1, concluimos la demostración aplicando el Teorema de continuidad.
\end{proof}

\section{Teoría de la probabilidad: Ley de los grandes números, formulación fuerte}
\begin{teorema}
Sean $A_n$, $n \in \mathds{N}$ variables aleatorias independientes, con esperanza $\mu$ e idénticamente distribuidas, es decir, obtenidas de la misma distribución. Entonces el valor medio de $A_n$, $\bar{\mu}$, converge casi seguramente a $\mu$:

$$
\bar{\mu}=\frac{1}{n}\sum_{n\in\mathds{N}} A_n \to^{c.s.} \mu,
$$

esto es, $P(\lim_{n\to\infty} \bar{\mu}=\mu) = 1$.

\end{teorema}

Para obtener una demostración un tanto más breve de éste teorema añadiremos la hipótesis de existencia del momento de orden 4 de $A_n$. Una demostración completa del teorema sin la hipótesis adicional se puede consultar en \cite{elLibro}.

\begin{lema}  \label{intercambio_suma}
Sean $A_n$,$ n \geq 1$ variables aleatorias no negativas, entonces: $$
E \left( \sum_{n \geq 1} A_n \right) = \sum_{n \geq 1} E(A_n).
$$

\end{lema}

\begin{lema}
En las misma condiciones de la ley de los grandes números, existe una constante $ K < \infty $ tal que para todo $ n \geq 0$:
$$
\mathds{E} \big( ( \bar{ \mu } - n \mu ) ^ 4 \big) \leq K n^2.
$$
\end{lema}

\begin{proof}

Sean 
$$
Z_k = A_k - \mu \quad y \quad T_n = Z_1 + Z_2 + ... + Z_n = \sum_{i=1}^{n} A_n - n\mu. 
$$

Entonces:

$$
\mathds{E} ( T_{n}^{4} ) = \mathds{E} ( \big( \sum_{i=1}^{n} Z_i \big) ^{4} ) = n\mathds{E}(Z_{1}^4)+3n(n-1)\mathds{E}(Z_1^2 Z_2^2) \leq Kn^2.
$$

donde en la segunda igualdad se ha empleado el desarrollo multinomial:

$$
(x_1+x_2+...+x_m)^n = \sum_{k_1+k_2+...+k_m=n} { n \choose k_1,k_2, ..., k_n} \prod_{1 \leq t \leq m} x_t^{k_t},
$$

con

$$
{ n \choose k_1,k_2, ..., k_n} = \frac{n!}{k_1!k_2! \dotsb k_n!}.
$$

Dado que $\mathds{E}(Z_k)=0 \quad \forall k$ y la independencia de las variables $Z_k$, se han cancelado todos los sumandos de la forma:

\begin{align*}
	\mathds{E} (Z_{i} Z_{j}^3 ) &=\mathds{E} (Z_{i}) \mathds{E} (Z_{j}^3) = 0, \quad 1 \leq i,j \leq n, \quad i \neq j, \\
	\mathds{E} ( Z_{i} Z_{j} Z_{k} Z_{l} ) &= \mathds{E} (Z_{i}) \mathds{E} (Z_{j}) \mathds{E} (Z_{k}) \mathds{E} (Z_{l}) = 0, \quad 1 \leq i,j,k,l \leq n, \quad i \neq j \neq k \neq l.
\end{align*}

y $K$ adecuadamente elegida, $K = 4 max{\mathds{E}(Z_1^4), \mathds{E}(Z_1^2)^2}$.
\end{proof}

Ya tenemos todos los rudimentos necesarios para proceder a demostrar el teorema de esta sección.

\begin{proof}[Demostración (Ley de los grandes números)]

Asumamos que $\mathds{E}_c( A_n^4) < \infty \quad \forall n$, aplicando el lema anterior:

$$
	\mathds{E} \big( ( \bar{ \mu } - \mu ) ^ 4 \big) \leq \frac{K}{n^{2}}.
$$

Ahora sea $Y_n = ( \bar{ \mu } - \mu ) ^ 4, \forall n \in \mathds{N}$ una variable aleatoria por la proposición \ref{funcion_de_va} y en particular es no negativa, luego podemos aplicar el lema \ref{intercambio_suma} en la siguiente cadena de igualdades:

$$
	\mathds{E} \big( \sum_{n \geq 1}( \bar{ \mu } - \mu ) ^ 4 \big) = \sum_{n \geq 1} \mathds{E} \big( ( \bar{ \mu } - \mu ) ^ 4 \big)  \leq K\sum_{n \geq 1}\frac{1}{n^{2}} < \infty,
$$

lo que implica:

$$
\abs{ \sum_{n \geq 1} \big( \bar{ \mu } - \mu \big)^ 4 } = \sum_{n \geq 1} \big( \bar{ \mu } - \mu \big)^ 4 < \infty \quad c.s. 
$$

Pero si una serie es convergente, entonces la sucesión de su término general de la serie converge a cero, por tanto:
$$
 \bar{ \mu } \to \mu \quad c.s.
$$
\end{proof}

\section{Teoría de la computación}

\subsection{Introducción}
Dado que el comportamiento completamente síncrono de un autómata celular como herramienta de modelado es una rareza, se han realizado numerosas investigaciones empíricas del autómata celular asíncrono. Sin embargo, los pocos análisis formales realizados o bien se refieren a ejemplos o a casos particulares de asíncronicidad. [Alguna citilla mona]. Así pues tomaremos el concepto más generalista de autómata celular m-asíncrono \cite{oraculo}, cuya idea principal es tener algún tipo de oráculo el cual en cada unidad discreta de tiempo dice las células que tienen que ser actualizadas. Dicho oráculo se implementa empleando una medida de probabilidad $\mu$ sobre subconjuntos de enteros d-dimensionales, $\mathds{Z}^{d}$. Notar que la definición con la que trataremos es la extensión a espacios multidimensionales de la dada en \cite{oraculo}.

\subsection{Autómata celular determinista}
Un autómata celular determinista es un sistema dinámico discreto consistente en un array $d$-dimensional de autómatas finitos, llamados células. Cada célula está conectada uniformemente a un vecindario formado por un número finito de células, y tiene un estado de un conjunto finito de estados. Actualiza su estado de acuerdo a una función de transición la cual determina el estado de una célula considerando su propio estado y el de su vecindario. 

\begin{defi}
Formalmente, la tupla $A=(\mathds{Z}^{d}, N, Q, f)$ es un autómata celular determinista, de ahora en adelante autómata celular, donde:

\begin{itemize}
\item $\mathds{Z} ^{d}$ es un espacio de células $d$-dimensional.
\item $Q$ el conjunto de estados posibles para cada célula.
\item $N \in (\mathds{Z}^{d})^{k}$ es el vecindario genérico de un autómata celular, esto es, para $N=(n_{1},...,n_{k})$, $a \in \mathds{Z} ^{d}$ célula, cada célula en $\{(a+n_{1},...,a+n_{k})\}$ es una célula vecina de $a$.
\item $f:Q^{k+1} \rightarrow Q$ es la función de transición local que define la transición de estado de cada célula como función de su propio estado y del estado de cada célula en su vecindario. 
\end{itemize}

\end{defi}

\begin{defi}
Una configuración es una función $g: \mathds{Z}^{d} \rightarrow Q$, la cual a cada punto del espacio $\mathds{Z}^{d}$ le asigna un estado del conjunto de estados $Q$, al conjunto de las configuración lo notaremos $Q^{\mathds{Z}^{d}}$. 
\end{defi}

\begin{defi}
La función de transición local, fijando una configuración g induce una función de transición global $F:Q^{\mathds{Z}^{d}} \rightarrow Q^{\mathds{Z}^{d}}$ definida como sigue:

\begin{equation*}
\forall x \in Q^{\mathds{Z}^{d}}, \quad \forall i \in \mathds{Z}^{d}, \quad F(x)(i) = f(x(i),x(i+n_{1}),...,x(i+n_{k}))
\end{equation*}

\end{defi}

\begin{defi}
Un autómata celular m-asíncrono C es una tupla $(A, \mu)$ donde: 
\begin{itemize}
\item A es un autómata celular 
\item $\mu$ es una medida de probabilidad sobre la $\sigma$-álgebra de Borel en $P(\mathds{Z}^{d})$
\end{itemize} 
\end{defi}

\begin{defi}
Para cada función de transición local $f$ y cada conjunto $\tau \in P(\mathds{Z}^{d})$, definimos la función de transición global $F:Q^{\mathds{Z}^{d}} \rightarrow Q^{\mathds{Z}^{d}}$ como sigue:

\begin{equation*}
	\forall x \in Q^{\mathds{Z}^{d}}, \quad \forall i \in \mathds{Z}^{d}, \qquad
	F_{\tau}(x)(i) = \left\{ \begin{array}{lcc}
             f(x(i),x(i+n_{1}),...,x(i+n_{k})) &   si  & i \in \tau ,\\
             \\ x(i) & si  & i \notin \tau .\\
             \end{array}
             \right.
\end{equation*}

Es decir, $F_{\tau}$ aplica la función de transición local solo sobre los elementos de $\tau \subset \mathds{Z}^{d}$. 
\end{defi}

Notar que cada célula $i \in \mathds{Z}^{d}$ es actualizada con probabilidad $\mu(U_{i})$.

Esta nueva definición de autómata celular m-asíncrono, incluye la de autómata celular síncrono. Fijada una $\sigma$-álgebra $\mathds{B}$ sobre $\mathds{Z}^{d}$ y sea $C_{0}=(A, \mu_{0})$ un autómata celular m-asíncrono donde $\mu_{0}: \mathds{B} \rightarrow [0,1]$ viene dada por: 

\begin{equation*}
	 \forall A \in P(\mathds{Z}^{d}), \qquad 
	 \mu_{0}(A) = \left\{ \begin{array}{lcc}
             \ 1 &   si  & \mathds{Z}^{d} \in A ,\\
             \\0 &   si  & \mathds{Z}^{d} \notin A .\\
             \end{array}
             \right.
\end{equation*}

De esta manera, $\mu_{0}(\{\mathds{Z}^{d}\})=1$ y por lo tanto, en cada instante de tiempo se aplicará la función de transición local sobre $\mathds{Z}^{d}$.

Por otro lado, también contiene el concepto de evolución totalmente asíncrona comentado en la introducción, esto es, en cada instante de tiempo solo de aplica la función de transición local a una sola célula. Consideramos ahora el autómata celular m-asíncrono $C_{1}=(A, \mu_{1})$ donde $\mu_{1}: \mathds{B} \rightarrow [0,1]$ verifica lo siguiente:

\begin{enumerate}
\item $\mu_{1}(U_{i}) > 0, \quad \forall i \in \mathds{Z}^{d}$.
\item $\mu_{1}(U_{i} \cap U_{j}) = 0, \quad \forall i \neq j, \quad i,j \in \mathds{Z}^{d}$.
\end{enumerate}

Así solo los conjuntos de la forma \{k\} $(k \in \mathds{Z}^{d})$ se les aplica la función de transición local.

Por último, contiene el concepto de evolución $\alpha$-asíncrona que nos interesa. Dado $C_{2}=(A, \mu_{2})$ un autómata celular m-asíncrono y sea $\alpha \in (0,1)$ la probabilidad con la que se actualizan las células, $\mu_{2}: \mathds{B} \rightarrow [0,1]$ satisfaciendo:

\begin{enumerate}
\item $\mu_{2}(U_{i}) = \alpha, \quad \forall i \in \mathds{Z}^{d}$.
\item $ \forall A \subseteq \mathds{Z}^{d} \quad$ finito,$\quad  \mu_{2} ( \bigcap_{a \in A} U_{a} ) = \prod_{a \in A} \mu_{2} ( U_{a} )$.
\end{enumerate}


\subsection{Juego de vida de Conway}

\begin{defi}
El juego de vida de Conway es un autómata celular síncrono:

\begin{equation}
C = (\mathds{Z}^{2} , N=\{(-1, 1), (0, 1), (1, 1), (-1, 0), (1, 0), (-1,-1), (0,-1), (1,-1) \}, Q=\{0,1\}, f)
\end{equation}
 
donde $f:\{0,1\}^{9} \rightarrow \{0,1\} $ viene dada por:

\begin{equation}
f(x)= \left\{ \begin{array}{lcc}
             1 &   si  & x_{0}=0 \quad y \quad \sum_{i=1}^{8} x_i = 3 \\
             \\ 1 & si & x_{0}=1 \quad y \quad \sum_{i=1}^{8} x_i \in \{2 ,3\} \\
             \\ 0 &  si  & \sum_{i=1}^{8} x_i \notin \{2, 3\} \
             \end{array}
   \right. 
\end{equation}
y $x = (x_{0}, x_{1}, ...,x_{8}) = (c,c+n_{1},...,c+n_{8})$ con $c \in \mathds{Z} ^{d}$ célula.
\end{defi}

\subsection{Juego de vida de Conway asíncrono}

\begin{defi}
\end{defi}


\section{Generadores de números aleatorios}

\section{Introducción}

Curiosamente, en las definiciones de los métodos de Monte Carlo no hay referencia explícita al empleo de la capacidades de cómputo de los ordenadores, sin embargo el gran desarrollo que han experimentado éstos desde el último tercio de siglo XX hasta nuestros días, los ha convertido en herramientas indispensables en las simulaciones Monte Carlo. La generación de números aleatorios ha experimentado también un importante crecimiento en las últimas décadas. En un principio los generadores de números aleatorios  más usados venían expresados por la siguiente ecuación recurrente:

\begin{equation} \label{cong}
I_{j+1} = aI_{j} +c \mod (m)
\end{equation}

donde $a$ es un entero positivo llamado multiplicador y $c$ es un número natural llamado incremento. Para $c \neq 0$, \ref{cong} es conocido por el nombre: generador lineal congruente de números aleatorios. Claramente, en $n<m$ pasos la ecuación \ref{cong} comienza a generar valores duplicados en el mismo orden, conocido ésto, se hacían elecciones particulares de $a,c$ y $m$ que obtuvieran en mayor periodo posible. Notar que la elección del valor inicial $I_{0}$ no es relevante, pues se generarán todos los naturales posibles entre $0$ y $m-1$ antes de la primera repetición. Una de las primeras debilidades que se encontraron en este tipo de generadores es que si $n$ números aleatorios se utilizan para pintar puntos en el espacio $[0,1]^{n}$, los puntos no tenderán a rellenar el espacio si no que se agruparán en planos de dimensión $n-1$ \cite{planos}. 

El desarrollo de nuevas técnicas de generación de números aleatorios, también se plasma en el desarrollo de baterías de test estadísticos para comprobar la eficacia de dichos generadores. Algunos de estos test estadísticos son:

\begin{itemize}
\item test1
\item test2
\end{itemize}

Emplearemos la batería de test \textit{Dieharder}, la cual está basada en los primeros test estadísticos propuestos por George Marsaglia en \textit{Diehard battery of tests} e incluye también los test desarrollados por el NIST (National Institute for Standards and Technology) \cite{dieharder}.

\section{Discusión}


\end{document}
